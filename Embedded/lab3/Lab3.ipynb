{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHGC0EFt3aZF"
      },
      "source": [
        "# Neural networks quantization\n",
        "\n",
        "Today we will deal with neural networks quantization!\n",
        "\n",
        "Remember that our goal is to reduce network size while keeping the accuracy high!\n",
        "\n",
        "For this purpose we will use Intel's OpenVINO and Neural Network Compression Framework (NNCF). Be aware that there are other frameworks to choose from: build-in PyTorch quantization, Brevitas from Xilinx, TensorRT and others.\n",
        "\n",
        "Use this link for OpenVINO reference and documentation: https://docs.openvino.ai/2023.0/home.html.\n",
        "\n",
        "First, install and import necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNKDyp4Ssmcw",
        "outputId": "14295079-e195-46e3-b850-9da539b869f1"
      },
      "outputs": [],
      "source": [
        "# !pip3 install openvino\n",
        "# !pip3 install nncf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1n_xcqlrsaMY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import nncf\n",
        "import openvino as ov\n",
        "import time\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "from nncf import NNCFConfig\n",
        "from nncf.torch import create_compressed_model, register_default_init_args\n",
        "from openvino.runtime.ie_api import CompiledModel\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, RandomRotation\n",
        "from typing import Union, List, Tuple, Any\n",
        "from abc import ABC, abstractmethod\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEGYhjeZ453d"
      },
      "source": [
        "Let's start with...\n",
        "\n",
        "##Quantizing Models Post-training\n",
        "\n",
        "Post-training model optimization is the process of applying special methods that transform the model into a more hardware-friendly representation without retraining or fine-tuning. The most popular and widely-spread method here is 8-bit post-training quantization because:\n",
        "\n",
        "- It is easy-to-use.\n",
        "- It does not hurt accuracy a lot.\n",
        "- It provides significant performance improvement.\n",
        "- It suites many hardware devices available in stock since most of them support 8-bit computation natively.\n",
        "\n",
        "8-bit integer quantization lowers the precision of weights and activations to 8 bits, which leads to significant reduction in the model footprint and significant improvements in inference speed.\n",
        "\n",
        "Source: https://docs.openvino.ai/2023.0/ptq_introduction.html.\n",
        "\n",
        "So, first, we need a model to quantize.\n",
        "Reuse the CNN model from Laboratory 1 (along with training loops, metrics, optimizers and loss function). You can also use the CNN defined below.\n",
        "\n",
        "Train it for 5 epochs with MNIST dataset. You should get around ~90% accuracy.\n",
        "\n",
        "Name the final trained model `CNN_MNIST`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cf507ijlfZgH"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6hbs9MatX7e3"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import tqdm\n",
        "import torch\n",
        "\n",
        "def train_or_test(model, data_generator, criterion, metric, mode='test', optimizer=None, update_period=None, device=device) -> Tuple[torch.nn.Module, float, float]:\n",
        "    # Change model mode to train or test\n",
        "    if mode == 'train':\n",
        "        model.train()\n",
        "    elif mode == 'test':\n",
        "        model.eval()\n",
        "    else:\n",
        "        raise RuntimeError(\"Unsupported mode.\")\n",
        "\n",
        "    # Move model to the specified device\n",
        "    model.to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    samples_num = 0\n",
        "\n",
        "    for i, (X, y) in tqdm.tqdm(enumerate(data_generator)):\n",
        "        # Convert tensors to the specified device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Process input data through the network\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        # Reset gradients and perform backpropagation if in training mode\n",
        "        if mode == 'train':\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = metric(y_pred, y)\n",
        "\n",
        "        total_loss += loss.item() * y_pred.size(0)\n",
        "        total_accuracy += accuracy.item() * y_pred.size(0)\n",
        "        samples_num += y_pred.size(0)\n",
        "\n",
        "    if samples_num == 0:\n",
        "        return model, 0.0, 0.0\n",
        "\n",
        "    return model, total_loss / samples_num, total_accuracy / samples_num\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_ekc_88ZX7e4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_and_display(model, train_loader, test_loader, criterion, metric, optimizer, device, num_epochs):\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model, train_loss, train_accuracy = train_or_test(\n",
        "            model, train_loader, criterion, metric, mode='train', optimizer=optimizer, device=device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Testing phase\n",
        "        model, test_loss, test_accuracy = train_or_test(\n",
        "            model, test_loader, criterion, metric, mode='test', device=device)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}  Train Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"  Test Loss: {test_loss:.4f}  Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # # Plot training history\n",
        "    # plt.figure(figsize=(12, 5))\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    # plt.plot(train_losses, label='Train Loss')\n",
        "    # plt.plot(test_losses, label='Test Loss')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Loss')\n",
        "    # plt.legend()\n",
        "    # plt.title('Loss History')\n",
        "\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    # plt.plot(train_accuracies, label='Train Accuracy')\n",
        "    # plt.plot(test_accuracies, label='Test Accuracy')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.legend()\n",
        "    # plt.title('Accuracy History')\n",
        "\n",
        "    # plt.show()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "l0E0q0jcX7e5"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "class BaseMetric(ABC):\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, y_pred, y_ref) -> Any:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "\n",
        "class AccuracyMetric(BaseMetric):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, y_pred: torch.Tensor, y_ref: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :param y_pred: tensor of shape (batch_size, num_of_classes) type float\n",
        "        :param y_ref: tensor with shape (batch_size,) and type Long\n",
        "        :return: scalar tensor with accuracy metric for batch\n",
        "        \"\"\"\n",
        "        # Get the predicted class (index with the highest probability) for each sample\n",
        "        predicted_classes = y_pred.argmax(dim=1)\n",
        "\n",
        "        # Compare the predicted classes to the reference labels\n",
        "        correct_predictions = (predicted_classes == y_ref).sum().item()\n",
        "\n",
        "        # Calculate the accuracy\n",
        "        accuracy = correct_predictions / y_ref.size(0)  # Divide by the batch size\n",
        "\n",
        "        return torch.tensor(accuracy)\n",
        "\n",
        "metric = AccuracyMetric()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "5BM76dcHX7e6"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),  # Random rotation up to 10 degrees\n",
        "    transforms.RandomCrop(28, padding=4),  # Random crop with padding\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
        "    transforms.ToTensor()  # Convert to a PyTorch tensor\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=train_transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=test_transform, download=True)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cnaAnDMsovx",
        "outputId": "85b29f5f-f8c8-4680-d231-1b1cffa63a75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1875it [01:32, 20.18it/s]\n",
            "313it [00:06, 45.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1:\n",
            "  Train Loss: 2.2196  Train Accuracy: 0.2595\n",
            "  Test Loss: 2.0168  Test Accuracy: 0.6207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self, input_shape, num_of_cls) -> None:\n",
        "        super().__init__()\n",
        "        ch_in = input_shape[0]\n",
        "        self.CNN = nn.Sequential(\n",
        "            # input shape = [1,28,28]\n",
        "            nn.Conv2d(ch_in, 32, 3, padding=(1, 1)),  # shape [32,28,28]\n",
        "            nn.BatchNorm2d(32),  # shape [32,28,28]\n",
        "            nn.ReLU(),  # shape [32,28,28]\n",
        "            nn.MaxPool2d(2, 2),  # shape [32,14,14]\n",
        "            nn.Conv2d(32, 64, 3, padding=(1, 1)),  # shape [64,14,14]\n",
        "            nn.BatchNorm2d(64),  # shape [64,14,14]\n",
        "            nn.ReLU(),  # shape [64,14,14]\n",
        "            nn.MaxPool2d(2, 2),  # shape [64,7,7]\n",
        "            nn.Conv2d(64, 128, 3),  # shape [128,5,5]\n",
        "            nn.BatchNorm2d(128),  # shape [128,5,5]\n",
        "            nn.ReLU(),  # shape [128,5,5]\n",
        "        )\n",
        "\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Flatten(), nn.Linear(128 * 5 * 5, num_of_cls), nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.CNN(x)\n",
        "        y = self.classification_head(x)\n",
        "        return y\n",
        "\n",
        "input_shape = (1, 28, 28)  # Assuming 28x28 images with 1 channel\n",
        "num_of_cls = 10  # Adjust the output size for your specific classification problem\n",
        "net = CNN(input_shape, num_of_cls)\n",
        "net = net.to(device)\n",
        "\n",
        "num_epochs = 1\n",
        "learning_rate = 0.001\n",
        "loss_fcn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "model = train_and_display(\n",
        "    net, train_loader, test_loader, loss_fcn, metric, optimizer, device, num_epochs\n",
        ")\n",
        "\n",
        "# You can alternatively load .pth file created during Lab1 with torch.load.\n",
        "# Use following code to upload files to colab:\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3rFVqhg8Yd0"
      },
      "source": [
        "Now - we will quantize this model to INT8.\n",
        "\n",
        "NNCF enables post-training quantization (PTQ) by adding the quantization layers into the model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers.\n",
        "\n",
        "By default PTQ uses an unannotated dataset to perform quantization. It uses representative dataset items to estimate the range of activation values in a network and then quantizes the network.\n",
        "\n",
        "Create an instance of `nncf.Dataset` class by passing two parameters:\n",
        "- data_source (PyTorch loader containing training samples)\n",
        "- transform_fn (to make data suitable for API).\n",
        "\n",
        "Call this instance `calibration_dataset`.\n",
        "\n",
        "Then, quantize `CNN_MNIST` model with `nncf.quantize()` function, which takes as input two parameters - the model and `calibration_dataset`. Call it `quantized_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyAzCv3Ta5nx",
        "outputId": "0a826c78-d068-47bc-ded7-f417a2f6f348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (CNN): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "  )\n",
            "  (classification_head): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=3200, out_features=10, bias=True)\n",
            "    (2): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWIfq-YZtbVY",
        "outputId": "89186492-b1e6-46f4-b537-b961b1dbef50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:nncf:Collecting tensor statistics |█               | 1 / 10\n",
            "INFO:nncf:Collecting tensor statistics |███             | 2 / 10\n",
            "INFO:nncf:Collecting tensor statistics |████            | 3 / 10\n",
            "INFO:nncf:Collecting tensor statistics |██████          | 4 / 10\n",
            "INFO:nncf:Collecting tensor statistics |████████        | 5 / 10\n",
            "INFO:nncf:Collecting tensor statistics |█████████       | 6 / 10\n",
            "INFO:nncf:Collecting tensor statistics |███████████     | 7 / 10\n",
            "INFO:nncf:Collecting tensor statistics |████████████    | 8 / 10\n",
            "INFO:nncf:Collecting tensor statistics |██████████████  | 9 / 10\n",
            "INFO:nncf:Collecting tensor statistics |████████████████| 10 / 10\n",
            "INFO:nncf:BatchNorm statistics adaptation |█               | 1 / 10\n",
            "INFO:nncf:BatchNorm statistics adaptation |███             | 2 / 10\n",
            "INFO:nncf:BatchNorm statistics adaptation |████            | 3 / 10\n",
            "INFO:nncf:BatchNorm statistics adaptation |██████          | 4 / 10\n",
            "INFO:nncf:BatchNorm statistics adaptation |████████        | 5 / 10\n",
            "INFO:nncf:BatchNorm statistics adaptation |█████████       | 6 / 10\n",
            "INFO:nncf:BatchNorm statistics adaptation |███████████     | 7 / 10\n",
            "INFO:nncf:BatchNorm statistics adaptation |████████████    | 8 / 10\n",
            "INFO:nncf:BatchNorm statistics adaptation |██████████████  | 9 / 10\n",
            "INFO:nncf:BatchNorm statistics adaptation |████████████████| 10 / 10\n"
          ]
        }
      ],
      "source": [
        "def transform_fn(data_item):\n",
        "    images, _ = data_item\n",
        "    return images\n",
        "\n",
        "calibration_dataset = nncf.Dataset(train_loader, transform_fn)\n",
        "quantized_model = nncf.quantize(model, calibration_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-khBK9_BfQ1"
      },
      "source": [
        "Finally, we will convert models to OpenVINO Intermediate Representation (IR) format.\n",
        "\n",
        "OpenVINO IR is the proprietary model format of OpenVINO. It is produced after converting a model with model conversion API. It translates the frequently used deep learning operations to their respective similar representation in OpenVINO and tunes them with the associated weights and biases from the trained model. The resulting IR contains two files:\n",
        "- `xml` - Describes the model topology.\n",
        "- `bin` - Contains the weights and binary data.\n",
        "\n",
        "To do that, we'll need `dummy_input` filled with random values and of size:\n",
        "\n",
        "`[batch_size, channel_number, image_shape[0], image_shape[1]]`\n",
        "\n",
        "Create `MNIST_fp32_ir` model with `ov.convert_model` that takes three parameters: the model, the dummy input and input size. Use `CNN_MNIST` model.\n",
        "\n",
        "Then, create `MNIST_int8_ir` model in the same way using `quantized_model`.\n",
        "\n",
        "Save both models to files (named `MNIST_fp32_ir.xml` and `MNIST_int8_ir.xml` respectively). Use `ov.save_model()` function.\n",
        "\n",
        "Finally - compile both models with `core.compile_model` function and use  `validate` function to calculate both models' accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CcYxirUfBV1",
        "outputId": "002b0d64-0e53-4b52-8cc1-5ae951836d61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "core = ov.Core()\n",
        "devices = core.available_devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'CPU'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "devices[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2BkQcrOUcn8R",
        "outputId": "ff967f1d-f1f3-42b1-eeea-661aad2b53ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\layers.py:336: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  return self._level_low.item()\n",
            "c:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\layers.py:344: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  return self._level_high.item()\n",
            "313it [00:01, 169.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FP 32 model acc=0.6207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "313it [00:01, 219.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INT 8 model acc=0.6174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import openvino as ov\n",
        "from openvino import CompiledModel\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Union\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "\n",
        "core = ov.Core()\n",
        "dummy_input = torch.randn(1, 1, 28, 28)\n",
        "\n",
        "MNIST_fp32_ir = ov.convert_model(model, example_input=dummy_input, input=[-1, 1, 28, 28])\n",
        "\n",
        "MNIST_int8_ir = ov.convert_model(quantized_model, example_input=dummy_input, input=[-1, 1, 28, 28])\n",
        "\n",
        "ov.save_model(MNIST_fp32_ir, \"mnist_fp32.xml\")  # Fill the filename\n",
        "ov.save_model(MNIST_int8_ir, \"mnist_int8.xml\")  # Fill the filename\n",
        "\n",
        "fp32_compiled_model = core.compile_model(MNIST_fp32_ir, \"CPU\")\n",
        "int8_compiled_model = core.compile_model(MNIST_int8_ir, \"CPU\")\n",
        "\n",
        "def validate(val_loader: torch.utils.data.DataLoader, model: Union[torch.nn.Module, CompiledModel], metric: BaseMetric):\n",
        "    # Switch to evaluate mode.\n",
        "    if not isinstance(model, CompiledModel):\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "\n",
        "    total_accuracy = 0\n",
        "    samples_num = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in tqdm.tqdm(enumerate(val_loader)):\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # Ensure the model is on the correct device\n",
        "            if not isinstance(model, CompiledModel):\n",
        "                model.to(device)\n",
        "\n",
        "            # Compute the output.\n",
        "            if isinstance(model, CompiledModel):\n",
        "                output_layer = model.output(0)\n",
        "                output = model(images)[output_layer]\n",
        "                output= torch.from_numpy(output)\n",
        "            else:\n",
        "                output = model(images)\n",
        "\n",
        "            # Measure accuracy and record loss.\n",
        "            accuracy = metric(output, target)\n",
        "            total_accuracy += accuracy.item() * target.shape[0]\n",
        "            samples_num += target.shape[0]\n",
        "\n",
        "    return total_accuracy / samples_num\n",
        "\n",
        "\n",
        "# Assuming you have a DataLoader for validation named 'val_loader'\n",
        "acc1 = validate(val_loader=test_loader, model=fp32_compiled_model, metric=AccuracyMetric())\n",
        "print(f'FP 32 model acc={acc1:.4f}')\n",
        "\n",
        "acc2 = validate(val_loader=test_loader, model=int8_compiled_model, metric=AccuracyMetric())\n",
        "print(f'INT 8 model acc={acc2:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpb_tq9yEKBg"
      },
      "source": [
        "Is INT8 model accuracy similar to FP32 model accuracy? We should hope so!\n",
        "\n",
        "But let's verify what we have saved in terms of memory resources and network throughput!\n",
        "\n",
        "First, check the size of OpenVINO IR binary files. You saved both of them on your drive. Is the INT8 model smaller?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have 244ko vs 120ko"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vp6wdbtFD6C"
      },
      "source": [
        "Then, use the following code to benchmark both models. Is INT8 model faster?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "313it [00:03, 94.01it/s] \n",
        "FP 32 model acc=0.6372\n",
        "313it [00:02, 146.47it/s]\n",
        "INT 8 model acc=0.6374\n",
        "\n",
        "INT 8 is faster !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPe2aECWFmqF"
      },
      "source": [
        "Note that we used very small network and we deal with very simple task. For bigger models and more complicated networks the perfomance and size differences can be even more significant!\n",
        "\n",
        "**EXTENSION EXERCISE**\n",
        "\n",
        "Read about `Quantizing with Accuracy Control` and try to use it for some pretrained network. Use `nncf.quantize_with_accuracy_control`. You can find pretrained networks with `torchvision.models`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCm0TMCCFMjq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpPQPePcGoDL"
      },
      "source": [
        "## Quantization-aware Training (QAT)\n",
        "\n",
        "Training-time model compression improves model performance by applying optimizations (such as quantization) during the training. The training process minimizes the loss associated with the lower-precision optimizations, so it is able to maintain the model’s accuracy while reducing its latency and memory footprint. Generally, training-time model optimization results in better model performance and accuracy than post-training optimization, but it can require more effort to set up.\n",
        "\n",
        "Quantization-aware Training is a popular method that allows quantizing a model and applying fine-tuning to restore accuracy degradation caused by quantization. In fact, this is the most accurate quantization method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ns4TqBHTM_"
      },
      "source": [
        "For this part, let's use a bit harder Dataset. For MNIST, PTQ method was enough, right?\n",
        "\n",
        "Train your CNN model on CIFAR10 dataset for 10-20 epochs (google it!). Use the same training loops, metrics, optimizers and loss function.\n",
        "\n",
        "Name the final trained model `CNN_CIFAR`, convert it to OpenVINO IR and save to an xml file.\n",
        "\n",
        "We start our QAT process with creating compressed models. Just use the following code (fill in the gaps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: .\\cifar10.tgz\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Dowload the dataset\n",
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
        "download_url(dataset_url, '.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract from archive\n",
        "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
        "    tar.extractall(path='./data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['test', 'train']\n",
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ],
      "source": [
        "data_dir = './data/cifar10'\n",
        "\n",
        "print(os.listdir(data_dir))\n",
        "classes = os.listdir(data_dir + \"/train\")\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(random_seed);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(45000, 5000)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_size = 5000\n",
        "train_size = len(dataset) - val_size\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "len(train_ds), len(val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "batch_size=32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
        "        \n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Cifar10CnnModel(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
        "\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(256*4*4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Cifar10CnnModel(\n",
              "  (network): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Flatten(start_dim=1, end_dim=-1)\n",
              "    (16): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (17): ReLU()\n",
              "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (19): ReLU()\n",
              "    (20): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Cifar10CnnModel()\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)\n",
        "to_device(model, device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = to_device(Cifar10CnnModel(), device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0], train_loss: 2.3153, val_loss: 2.3023, val_acc: 0.0961\n",
            "Epoch [1], train_loss: 2.2996, val_loss: 2.3037, val_acc: 0.1015\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have already defined train_ds and val_ds\n",
        "# Create a smaller subset of the training dataset (10% of the original)\n",
        "subset_percentage = 0.001\n",
        "subset_size = int(len(train_ds) * subset_percentage)\n",
        "train_ds_subset, _ = torch.utils.data.random_split(train_ds, [subset_size, len(train_ds) - subset_size])\n",
        "\n",
        "# DataLoader for the subset of the training dataset\n",
        "train_dl_subset = DataLoader(train_ds_subset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Train the model with the subset of the training dataset\n",
        "num_epochs = 2\n",
        "lr = 0.001\n",
        "history = fit(num_epochs, lr, model, train_dl_subset, val_dl, opt_func=torch.optim.Adam)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "xdNDbJp09l3Q"
      },
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "Unexpected object provided for input. Expected tuple, Shape, PartialShape, Type or str. Got <class 'torch.Tensor'>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Informatique\\EmbeddedAI\\Embedded\\lab3\\Lab3.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# SAVE floating point model converted to OpenVINO IR\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m)  \u001b[39m# Replace with your actual input shape\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m CIFAR_fp32_ir \u001b[39m=\u001b[39m ov\u001b[39m.\u001b[39;49mconvert_model(CNN_CIFAR, dummy_input)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ov\u001b[39m.\u001b[39msave_model(CIFAR_fp32_ir, \u001b[39m'\u001b[39m\u001b[39mpath/to/your/fp32_model.xml\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Replace with your desired file path\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Compress model\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\openvino\\tools\\ovc\\convert.py:101\u001b[0m, in \u001b[0;36mconvert_model\u001b[1;34m(input_model, input, output, example_input, extension, verbose, share_weights)\u001b[0m\n\u001b[0;32m     99\u001b[0m logger_state \u001b[39m=\u001b[39m get_logger_state()\n\u001b[0;32m    100\u001b[0m cli_parser \u001b[39m=\u001b[39m get_all_cli_parser()\n\u001b[1;32m--> 101\u001b[0m ov_model, _ \u001b[39m=\u001b[39m _convert(cli_parser, params, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    102\u001b[0m restore_logger_state(logger_state)\n\u001b[0;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m ov_model\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py:524\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(cli_parser, args, python_api_used)\u001b[0m\n\u001b[0;32m    522\u001b[0m send_conversion_result(\u001b[39m'\u001b[39m\u001b[39mfail\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m python_api_used:\n\u001b[1;32m--> 524\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    525\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    526\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, argv\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py:476\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(cli_parser, args, python_api_used)\u001b[0m\n\u001b[0;32m    472\u001b[0m argv\u001b[39m.\u001b[39mis_python_api_used \u001b[39m=\u001b[39m python_api_used\n\u001b[0;32m    474\u001b[0m argv\u001b[39m.\u001b[39mframework \u001b[39m=\u001b[39m model_framework\n\u001b[1;32m--> 476\u001b[0m ov_model \u001b[39m=\u001b[39m driver(argv, {\u001b[39m\"\u001b[39;49m\u001b[39mconversion_parameters\u001b[39;49m\u001b[39m\"\u001b[39;49m: non_default_params})\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m inp_model_is_object \u001b[39mand\u001b[39;00m model_framework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpaddle\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    479\u001b[0m     \u001b[39mif\u001b[39;00m paddle_runtime_converter:\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py:226\u001b[0m, in \u001b[0;36mdriver\u001b[1;34m(argv, non_default_params)\u001b[0m\n\u001b[0;32m    222\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mstr\u001b[39m(non_default_params))\n\u001b[0;32m    224\u001b[0m start_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m--> 226\u001b[0m ov_model \u001b[39m=\u001b[39m moc_emit_ir(prepare_ir(argv), argv)\n\u001b[0;32m    228\u001b[0m \u001b[39mif\u001b[39;00m argv\u001b[39m.\u001b[39mverbose:\n\u001b[0;32m    229\u001b[0m     elapsed_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py:152\u001b[0m, in \u001b[0;36mprepare_ir\u001b[1;34m(argv)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_ir\u001b[39m(argv: argparse\u001b[39m.\u001b[39mNamespace):\n\u001b[1;32m--> 152\u001b[0m     argv \u001b[39m=\u001b[39m arguments_post_parsing(argv)\n\u001b[0;32m    153\u001b[0m     t \u001b[39m=\u001b[39m tm\u001b[39m.\u001b[39mTelemetry()\n\u001b[0;32m    155\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(argv\u001b[39m.\u001b[39minput_model, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(argv\u001b[39m.\u001b[39minput_model) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py:96\u001b[0m, in \u001b[0;36marguments_post_parsing\u001b[1;34m(argv)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m is_verbose(argv):\n\u001b[0;32m     94\u001b[0m     print_argv(argv)\n\u001b[1;32m---> 96\u001b[0m params_parsing(argv)\n\u001b[0;32m     97\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mPlaceholder shapes : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(argv\u001b[39m.\u001b[39mplaceholder_shapes))\n\u001b[0;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(argv, \u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m argv\u001b[39m.\u001b[39moutput \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py:317\u001b[0m, in \u001b[0;36mparams_parsing\u001b[1;34m(argv)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[39mParses params passed to convert_model and wraps resulting values into dictionaries or lists.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39mAfter working of this method following values are set in argv:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39m:param argv: MO arguments\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[39m# Parse input to list of InputCutInfo\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m inputs \u001b[39m=\u001b[39m input_to_input_cut_info(argv\u001b[39m.\u001b[39;49minput)\n\u001b[0;32m    319\u001b[0m \u001b[39m# Make list of input names\u001b[39;00m\n\u001b[0;32m    320\u001b[0m input_names_list \u001b[39m=\u001b[39m []\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\openvino\\tools\\ovc\\cli_parser.py:163\u001b[0m, in \u001b[0;36minput_to_input_cut_info\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m res_list\n\u001b[0;32m    162\u001b[0m \u001b[39m# Case when single type or value is set, or unknown object\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m [single_input_to_input_cut_info(\u001b[39minput\u001b[39;49m)]\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\openvino\\tools\\ovc\\cli_parser.py:90\u001b[0m, in \u001b[0;36msingle_input_to_input_cut_info\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m _InputCutInfo(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39minput\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m# pylint: disable=no-member\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m# We don't expect here single unnamed value. If list of int is set it is considered as shape.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39m# Setting of value is expected only using InputCutInfo or string analog.\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected object provided for input. Expected tuple, Shape, PartialShape, Type or str. Got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m)))\n",
            "\u001b[1;31mException\u001b[0m: Unexpected object provided for input. Expected tuple, Shape, PartialShape, Type or str. Got <class 'torch.Tensor'>"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import openvino as ov\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Replace this with your actual model\n",
        "CNN_CIFAR = model\n",
        "\n",
        "# SAVE floating point model converted to OpenVINO IR\n",
        "dummy_input = torch.randn(1, 3, 32, 32)  # Replace with your actual input shape\n",
        "CIFAR_fp32_ir = ov.convert_model(CNN_CIFAR, dummy_input)\n",
        "ov.save_model(CIFAR_fp32_ir, 'path/to/your/fp32_model.xml')  # Replace with your desired file path\n",
        "\n",
        "# Compress model\n",
        "nncf_config_dict = {\n",
        "    \"input_info\": {\"sample_size\": [1, 3, 32, 32]},  # Adjust with actual input size\n",
        "    \"compression\": {\n",
        "        \"algorithm\": \"quantization\",\n",
        "    },\n",
        "}\n",
        "nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n",
        "nncf_config = register_default_init_args(nncf_config, train_loader)\n",
        "compression_ctrl, CNN_CIFAR_int8 = create_compressed_model(CNN_CIFAR, nncf_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wetn5RnaZzGx"
      },
      "source": [
        "We have our CIFAR CNN model ready to QAT. So... Just train it!\n",
        "\n",
        "Use your `training` function to train `CNN_CIFAR_int8` model for one more epoch!\n",
        "\n",
        "Thanks to OpenVINO API, after creating compressed model all we need to do is to continue training on INT8 model :) We call this process fine-tuning. It is applied to futher improve quantized model accuracy! Normally, several epochs of tuning are required with a small learning rate, the same that is usually used at the end of the training of the original model. No other changes in the training pipeline are required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj2Va0bwaW0i"
      },
      "outputs": [],
      "source": [
        "CNN_CIFAR_int8_finetuned, history = training( ... ) # just one epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKp3LbjUbBg-"
      },
      "source": [
        "Convert fine-tuned model to OpenVINO IR, save it to xml and verify both `CIFAR_fp32_ir` and `CIFAR_int8_ir` sizes.\n",
        "\n",
        "Is the INT8 network smaller?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MigLfK-pLVqA"
      },
      "outputs": [],
      "source": [
        "core = ov.Core()\n",
        "devices = core.available_devices\n",
        "dummy_input = ...\n",
        "CIFAR_int8_ir = ov.convert_model( ... )\n",
        "ov.save_model( ... )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS9CujUULnz0"
      },
      "outputs": [],
      "source": [
        "!ls -lh ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbvVrwcCbZH_"
      },
      "source": [
        "Finally - compile models, validate and benchmark them.\n",
        "\n",
        "Did accuracy decreased?\n",
        "Is INT8 model faster?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1gbA8jbLidw"
      },
      "outputs": [],
      "source": [
        "fp32_cifar_compiled_model = core.compile_model( ... )\n",
        "int8_cifar_compiled_model = core.compile_model( ... )\n",
        "acc1 = validate( ... )\n",
        "print( ... )\n",
        "acc2 = validate( ... )\n",
        "print( ... )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pmPBzNrbm7r"
      },
      "outputs": [],
      "source": [
        "def parse_benchmark_output(benchmark_output: str):\n",
        "    \"\"\"Prints the output from benchmark_app in human-readable format\"\"\"\n",
        "    parsed_output = [line for line in benchmark_output if 'FPS' in line]\n",
        "    print(*parsed_output, sep='\\n')\n",
        "\n",
        "\n",
        "print('Benchmark FP32 model on CPU')\n",
        "benchmark_output = ! benchmark_app -m CIFAR_fp32_ir.xml -d CPU -api async -t 15 -shape \"[1, 3, 32, 32]\"\n",
        "parse_benchmark_output(benchmark_output)\n",
        "\n",
        "print('Benchmark INT8 model on CPU')\n",
        "benchmark_output = ! benchmark_app -m CIFAR_int8_ir.xml -d CPU -api async -t 15 -shape \"[1, 3, 32, 32]\"\n",
        "parse_benchmark_output(benchmark_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wku_Ng6WcDdI"
      },
      "source": [
        "**EXTENSION EXERCISE**\n",
        "\n",
        "Compare PTQ and QAT. Create CNN model and:\n",
        "- train it for 20 epochs and save as `CNN_long.pth`\n",
        "- train it for 15 epochs and save as `CNN_short.pth`\n",
        "\n",
        "Then, apply PTQ on `CNN_long.pth` model and QAT (for 5 epochs) on `CNN_short.pth`. Compare the resulting models (in terms of accuracy, size and FPS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY_MYT8GFR-u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

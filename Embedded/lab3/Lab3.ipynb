{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHGC0EFt3aZF"
      },
      "source": [
        "# Neural networks quantization\n",
        "\n",
        "Today we will deal with neural networks quantization!\n",
        "\n",
        "Remember that our goal is to reduce network size while keeping the accuracy high!\n",
        "\n",
        "For this purpose we will use Intel's OpenVINO and Neural Network Compression Framework (NNCF). Be aware that there are other frameworks to choose from: build-in PyTorch quantization, Brevitas from Xilinx, TensorRT and others.\n",
        "\n",
        "Use this link for OpenVINO reference and documentation: https://docs.openvino.ai/2023.0/home.html.\n",
        "\n",
        "First, install and import necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNKDyp4Ssmcw",
        "outputId": "14295079-e195-46e3-b850-9da539b869f1"
      },
      "outputs": [],
      "source": [
        "# !pip3 install openvino\n",
        "# !pip3 install nncf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1n_xcqlrsaMY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, openvino\n",
            "WARNING:nncf:NNCF provides best results with torch==2.1.0, while current torch version is 2.1.1+cu118. If you encounter issues, consider switching to torch==2.1.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import nncf\n",
        "import openvino as ov\n",
        "import time\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "from nncf import NNCFConfig\n",
        "from nncf.torch import create_compressed_model, register_default_init_args\n",
        "from openvino.runtime.ie_api import CompiledModel\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, RandomRotation\n",
        "from typing import Union, List, Tuple, Any\n",
        "from abc import ABC, abstractmethod\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEGYhjeZ453d"
      },
      "source": [
        "Let's start with...\n",
        "\n",
        "##Quantizing Models Post-training\n",
        "\n",
        "Post-training model optimization is the process of applying special methods that transform the model into a more hardware-friendly representation without retraining or fine-tuning. The most popular and widely-spread method here is 8-bit post-training quantization because:\n",
        "\n",
        "- It is easy-to-use.\n",
        "- It does not hurt accuracy a lot.\n",
        "- It provides significant performance improvement.\n",
        "- It suites many hardware devices available in stock since most of them support 8-bit computation natively.\n",
        "\n",
        "8-bit integer quantization lowers the precision of weights and activations to 8 bits, which leads to significant reduction in the model footprint and significant improvements in inference speed.\n",
        "\n",
        "Source: https://docs.openvino.ai/2023.0/ptq_introduction.html.\n",
        "\n",
        "So, first, we need a model to quantize.\n",
        "Reuse the CNN model from Laboratory 1 (along with training loops, metrics, optimizers and loss function). You can also use the CNN defined below.\n",
        "\n",
        "Train it for 5 epochs with MNIST dataset. You should get around ~90% accuracy.\n",
        "\n",
        "Name the final trained model `CNN_MNIST`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cf507ijlfZgH"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6hbs9MatX7e3"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import tqdm\n",
        "import torch\n",
        "\n",
        "def train_or_test(model, data_generator, criterion, metric, mode='test', optimizer=None, update_period=None, device=device) -> Tuple[torch.nn.Module, float, float]:\n",
        "    # Change model mode to train or test\n",
        "    if mode == 'train':\n",
        "        model.train()\n",
        "    elif mode == 'test':\n",
        "        model.eval()\n",
        "    else:\n",
        "        raise RuntimeError(\"Unsupported mode.\")\n",
        "\n",
        "    # Move model to the specified device\n",
        "    model.to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    samples_num = 0\n",
        "\n",
        "    for i, (X, y) in tqdm.tqdm(enumerate(data_generator)):\n",
        "        # Convert tensors to the specified device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Process input data through the network\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        # Reset gradients and perform backpropagation if in training mode\n",
        "        if mode == 'train':\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = metric(y_pred, y)\n",
        "\n",
        "        total_loss += loss.item() * y_pred.size(0)\n",
        "        total_accuracy += accuracy.item() * y_pred.size(0)\n",
        "        samples_num += y_pred.size(0)\n",
        "\n",
        "    if samples_num == 0:\n",
        "        return model, 0.0, 0.0\n",
        "\n",
        "    return model, total_loss / samples_num, total_accuracy / samples_num\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_ekc_88ZX7e4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_and_display(model, train_loader, test_loader, criterion, metric, optimizer, device, num_epochs):\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model, train_loss, train_accuracy = train_or_test(\n",
        "            model, train_loader, criterion, metric, mode='train', optimizer=optimizer, device=device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Testing phase\n",
        "        model, test_loss, test_accuracy = train_or_test(\n",
        "            model, test_loader, criterion, metric, mode='test', device=device)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}  Train Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"  Test Loss: {test_loss:.4f}  Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # # Plot training history\n",
        "    # plt.figure(figsize=(12, 5))\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    # plt.plot(train_losses, label='Train Loss')\n",
        "    # plt.plot(test_losses, label='Test Loss')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Loss')\n",
        "    # plt.legend()\n",
        "    # plt.title('Loss History')\n",
        "\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    # plt.plot(train_accuracies, label='Train Accuracy')\n",
        "    # plt.plot(test_accuracies, label='Test Accuracy')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.legend()\n",
        "    # plt.title('Accuracy History')\n",
        "\n",
        "    # plt.show()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l0E0q0jcX7e5"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "class BaseMetric(ABC):\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, y_pred, y_ref) -> Any:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "\n",
        "class AccuracyMetric(BaseMetric):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, y_pred: torch.Tensor, y_ref: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :param y_pred: tensor of shape (batch_size, num_of_classes) type float\n",
        "        :param y_ref: tensor with shape (batch_size,) and type Long\n",
        "        :return: scalar tensor with accuracy metric for batch\n",
        "        \"\"\"\n",
        "        # Get the predicted class (index with the highest probability) for each sample\n",
        "        predicted_classes = y_pred.argmax(dim=1)\n",
        "\n",
        "        # Compare the predicted classes to the reference labels\n",
        "        correct_predictions = (predicted_classes == y_ref).sum().item()\n",
        "\n",
        "        # Calculate the accuracy\n",
        "        accuracy = correct_predictions / y_ref.size(0)  # Divide by the batch size\n",
        "\n",
        "        return torch.tensor(accuracy)\n",
        "\n",
        "metric = AccuracyMetric()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5BM76dcHX7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:03<00:00, 3042614.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 3208892.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3019701.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),  # Random rotation up to 10 degrees\n",
        "    transforms.RandomCrop(28, padding=4),  # Random crop with padding\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
        "    transforms.ToTensor()  # Convert to a PyTorch tensor\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=train_transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=test_transform, download=True)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cnaAnDMsovx",
        "outputId": "85b29f5f-f8c8-4680-d231-1b1cffa63a75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1875it [00:47, 39.74it/s]\n",
            "313it [00:02, 126.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1:\n",
            "  Train Loss: 2.2201  Train Accuracy: 0.2459\n",
            "  Test Loss: 2.0170  Test Accuracy: 0.5212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self, input_shape, num_of_cls) -> None:\n",
        "        super().__init__()\n",
        "        ch_in = input_shape[0]\n",
        "        self.CNN = nn.Sequential(\n",
        "            # input shape = [1,28,28]\n",
        "            nn.Conv2d(ch_in, 32, 3, padding=(1, 1)),  # shape [32,28,28]\n",
        "            nn.BatchNorm2d(32),  # shape [32,28,28]\n",
        "            nn.ReLU(),  # shape [32,28,28]\n",
        "            nn.MaxPool2d(2, 2),  # shape [32,14,14]\n",
        "            nn.Conv2d(32, 64, 3, padding=(1, 1)),  # shape [64,14,14]\n",
        "            nn.BatchNorm2d(64),  # shape [64,14,14]\n",
        "            nn.ReLU(),  # shape [64,14,14]\n",
        "            nn.MaxPool2d(2, 2),  # shape [64,7,7]\n",
        "            nn.Conv2d(64, 128, 3),  # shape [128,5,5]\n",
        "            nn.BatchNorm2d(128),  # shape [128,5,5]\n",
        "            nn.ReLU(),  # shape [128,5,5]\n",
        "        )\n",
        "\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Flatten(), nn.Linear(128 * 5 * 5, num_of_cls), nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.CNN(x)\n",
        "        y = self.classification_head(x)\n",
        "        return y\n",
        "\n",
        "input_shape = (1, 28, 28)  # Assuming 28x28 images with 1 channel\n",
        "num_of_cls = 10  # Adjust the output size for your specific classification problem\n",
        "net = CNN(input_shape, num_of_cls)\n",
        "net = net.to(device)\n",
        "\n",
        "num_epochs = 1\n",
        "learning_rate = 0.001\n",
        "loss_fcn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "model = train_and_display(\n",
        "    net, train_loader, test_loader, loss_fcn, metric, optimizer, device, num_epochs\n",
        ")\n",
        "\n",
        "# You can alternatively load .pth file created during Lab1 with torch.load.\n",
        "# Use following code to upload files to colab:\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3rFVqhg8Yd0"
      },
      "source": [
        "Now - we will quantize this model to INT8.\n",
        "\n",
        "NNCF enables post-training quantization (PTQ) by adding the quantization layers into the model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers.\n",
        "\n",
        "By default PTQ uses an unannotated dataset to perform quantization. It uses representative dataset items to estimate the range of activation values in a network and then quantizes the network.\n",
        "\n",
        "Create an instance of `nncf.Dataset` class by passing two parameters:\n",
        "- data_source (PyTorch loader containing training samples)\n",
        "- transform_fn (to make data suitable for API).\n",
        "\n",
        "Call this instance `calibration_dataset`.\n",
        "\n",
        "Then, quantize `CNN_MNIST` model with `nncf.quantize()` function, which takes as input two parameters - the model and `calibration_dataset`. Call it `quantized_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyAzCv3Ta5nx",
        "outputId": "0a826c78-d068-47bc-ded7-f417a2f6f348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (CNN): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "  )\n",
            "  (classification_head): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=3200, out_features=10, bias=True)\n",
            "    (2): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWIfq-YZtbVY",
        "outputId": "89186492-b1e6-46f4-b537-b961b1dbef50"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for Jupyter \n",
              "support\n",
              "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
              "</pre>\n"
            ],
            "text/plain": [
              "c:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for Jupyter \n",
              "support\n",
              "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:nncf:Compiling and loading torch extension: quantized_functions_cuda...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:383: UserWarning: Error checking compiler version for cl: [WinError 2] Le fichier spécifié est introuvable\n",
            "  warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA is available for PyTorch, but NNCF could not compile GPU quantization extensions. Make sure that you have installed CUDA development tools (see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html for guidance) and that 'nvcc' is available on your system's PATH variable.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\extensions.py:86\u001b[0m, in \u001b[0;36mQuantizedFunctionsCUDALoader.load\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mcpp_extension\u001b[39m.\u001b[39;49mload(\n\u001b[0;32m     87\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mname(),\n\u001b[0;32m     88\u001b[0m         CUDA_EXT_SRC_LIST,\n\u001b[0;32m     89\u001b[0m         extra_include_paths\u001b[39m=\u001b[39;49mEXT_INCLUDE_DIRS,\n\u001b[0;32m     90\u001b[0m         build_directory\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_build_dir(),\n\u001b[0;32m     91\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     92\u001b[0m     )\n\u001b[0;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m ExtensionLoaderTimeoutException \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1308\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m \u001b[39mLoads a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[39m    ...     verbose=True)\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m-> 1308\u001b[0m \u001b[39mreturn\u001b[39;00m _jit_compile(\n\u001b[0;32m   1309\u001b[0m     name,\n\u001b[0;32m   1310\u001b[0m     [sources] \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(sources, \u001b[39mstr\u001b[39;49m) \u001b[39melse\u001b[39;49;00m sources,\n\u001b[0;32m   1311\u001b[0m     extra_cflags,\n\u001b[0;32m   1312\u001b[0m     extra_cuda_cflags,\n\u001b[0;32m   1313\u001b[0m     extra_ldflags,\n\u001b[0;32m   1314\u001b[0m     extra_include_paths,\n\u001b[0;32m   1315\u001b[0m     build_directory \u001b[39mor\u001b[39;49;00m _get_build_directory(name, verbose),\n\u001b[0;32m   1316\u001b[0m     verbose,\n\u001b[0;32m   1317\u001b[0m     with_cuda,\n\u001b[0;32m   1318\u001b[0m     is_python_module,\n\u001b[0;32m   1319\u001b[0m     is_standalone,\n\u001b[0;32m   1320\u001b[0m     keep_intermediates\u001b[39m=\u001b[39;49mkeep_intermediates)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1710\u001b[0m, in \u001b[0;36m_jit_compile\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[0;32m   1708\u001b[0m             sources \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(hipified_sources)\n\u001b[1;32m-> 1710\u001b[0m         _write_ninja_file_and_build_library(\n\u001b[0;32m   1711\u001b[0m             name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1712\u001b[0m             sources\u001b[39m=\u001b[39;49msources,\n\u001b[0;32m   1713\u001b[0m             extra_cflags\u001b[39m=\u001b[39;49mextra_cflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[0;32m   1714\u001b[0m             extra_cuda_cflags\u001b[39m=\u001b[39;49mextra_cuda_cflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[0;32m   1715\u001b[0m             extra_ldflags\u001b[39m=\u001b[39;49mextra_ldflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[0;32m   1716\u001b[0m             extra_include_paths\u001b[39m=\u001b[39;49mextra_include_paths \u001b[39mor\u001b[39;49;00m [],\n\u001b[0;32m   1717\u001b[0m             build_directory\u001b[39m=\u001b[39;49mbuild_directory,\n\u001b[0;32m   1718\u001b[0m             verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1719\u001b[0m             with_cuda\u001b[39m=\u001b[39;49mwith_cuda,\n\u001b[0;32m   1720\u001b[0m             is_standalone\u001b[39m=\u001b[39;49mis_standalone)\n\u001b[0;32m   1721\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1800\u001b[0m, in \u001b[0;36m_write_ninja_file_and_build_library\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\u001b[0m\n\u001b[0;32m   1799\u001b[0m     with_cuda \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\u001b[39mmap\u001b[39m(_is_cuda_file, sources))\n\u001b[1;32m-> 1800\u001b[0m extra_ldflags \u001b[39m=\u001b[39m _prepare_ldflags(\n\u001b[0;32m   1801\u001b[0m     extra_ldflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[0;32m   1802\u001b[0m     with_cuda,\n\u001b[0;32m   1803\u001b[0m     verbose,\n\u001b[0;32m   1804\u001b[0m     is_standalone)\n\u001b[0;32m   1805\u001b[0m build_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(build_directory, \u001b[39m'\u001b[39m\u001b[39mbuild.ninja\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1893\u001b[0m, in \u001b[0;36m_prepare_ldflags\u001b[1;34m(extra_ldflags, with_cuda, verbose, is_standalone)\u001b[0m\n\u001b[0;32m   1892\u001b[0m \u001b[39mif\u001b[39;00m IS_WINDOWS:\n\u001b[1;32m-> 1893\u001b[0m     extra_ldflags\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/LIBPATH:\u001b[39m\u001b[39m{\u001b[39;00m_join_cuda_home(\u001b[39m\"\u001b[39;49m\u001b[39mlib\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mx64\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1894\u001b[0m     extra_ldflags\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mcudart.lib\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2416\u001b[0m, in \u001b[0;36m_join_cuda_home\u001b[1;34m(*paths)\u001b[0m\n\u001b[0;32m   2415\u001b[0m \u001b[39mif\u001b[39;00m CUDA_HOME \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCUDA_HOME environment variable is not set. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2417\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mPlease set it to your CUDA install root.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2418\u001b[0m \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(CUDA_HOME, \u001b[39m*\u001b[39mpaths)\n",
            "\u001b[1;31mOSError\u001b[0m: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Informatique\\EmbeddedAI\\Embedded\\lab3\\Lab3.ipynb Cell 13\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m images\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m calibration_dataset \u001b[39m=\u001b[39m nncf\u001b[39m.\u001b[39mDataset(train_loader, transform_fn)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m quantized_model \u001b[39m=\u001b[39m nncf\u001b[39m.\u001b[39;49mquantize(model, calibration_dataset)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\quantization\\quantize_model.py:137\u001b[0m, in \u001b[0;36mquantize\u001b[1;34m(model, calibration_dataset, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_parameters)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mif\u001b[39;00m backend \u001b[39m==\u001b[39m BackendType\u001b[39m.\u001b[39mTORCH:\n\u001b[0;32m    135\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnncf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantize_model\u001b[39;00m \u001b[39mimport\u001b[39;00m quantize_impl\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m quantize_impl(\n\u001b[0;32m    138\u001b[0m         model,\n\u001b[0;32m    139\u001b[0m         calibration_dataset,\n\u001b[0;32m    140\u001b[0m         preset,\n\u001b[0;32m    141\u001b[0m         target_device,\n\u001b[0;32m    142\u001b[0m         subset_size,\n\u001b[0;32m    143\u001b[0m         fast_bias_correction,\n\u001b[0;32m    144\u001b[0m         model_type,\n\u001b[0;32m    145\u001b[0m         ignored_scope,\n\u001b[0;32m    146\u001b[0m         advanced_parameters,\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    149\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported type of backend: \u001b[39m\u001b[39m{\u001b[39;00mbackend\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\quantize_model.py:125\u001b[0m, in \u001b[0;36mquantize_impl\u001b[1;34m(model, calibration_dataset, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_parameters)\u001b[0m\n\u001b[0;32m    113\u001b[0m nncf_network \u001b[39m=\u001b[39m create_nncf_network(copied_model\u001b[39m.\u001b[39meval(), calibration_dataset)\n\u001b[0;32m    115\u001b[0m quantization_algorithm \u001b[39m=\u001b[39m PostTrainingQuantization(\n\u001b[0;32m    116\u001b[0m     preset\u001b[39m=\u001b[39mpreset,\n\u001b[0;32m    117\u001b[0m     target_device\u001b[39m=\u001b[39mtarget_device,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m     advanced_parameters\u001b[39m=\u001b[39madvanced_parameters,\n\u001b[0;32m    123\u001b[0m )\n\u001b[1;32m--> 125\u001b[0m quantized_model \u001b[39m=\u001b[39m quantization_algorithm\u001b[39m.\u001b[39;49mapply(\n\u001b[0;32m    126\u001b[0m     nncf_network, nncf_network\u001b[39m.\u001b[39;49mnncf\u001b[39m.\u001b[39;49mget_graph(), dataset\u001b[39m=\u001b[39;49mcalibration_dataset\n\u001b[0;32m    127\u001b[0m )\n\u001b[0;32m    129\u001b[0m quantized_model\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39mdisable_dynamic_graph_building()\n\u001b[0;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m quantized_model\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\quantization\\algorithms\\post_training\\algorithm.py:102\u001b[0m, in \u001b[0;36mPostTrainingQuantization.apply\u001b[1;34m(self, model, graph, statistic_points, dataset)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mif\u001b[39;00m statistic_points:\n\u001b[0;32m    100\u001b[0m     step_index_to_statistics \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m: statistic_points}\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pipeline\u001b[39m.\u001b[39;49mrun_from_step(model, dataset, graph, \u001b[39m0\u001b[39;49m, step_index_to_statistics)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\quantization\\algorithms\\pipeline.py:164\u001b[0m, in \u001b[0;36mPipeline.run_from_step\u001b[1;34m(self, model, dataset, graph, start_step_index, step_index_to_statistics)\u001b[0m\n\u001b[0;32m    161\u001b[0m         step_statistics \u001b[39m=\u001b[39m collect_statistics(statistic_points, step_model, step_graph, dataset)\n\u001b[0;32m    163\u001b[0m     \u001b[39m# Run current pipeline step\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     step_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_step(step_index, step_statistics, step_model, step_graph)\n\u001b[0;32m    166\u001b[0m     step_graph \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# We should rebuild the graph for the next pipeline step\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m step_model\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\quantization\\algorithms\\pipeline.py:117\u001b[0m, in \u001b[0;36mPipeline.run_step\u001b[1;34m(self, step_index, step_statistics, model, graph)\u001b[0m\n\u001b[0;32m    115\u001b[0m pipeline_step \u001b[39m=\u001b[39m pipeline_steps[step_index]\n\u001b[0;32m    116\u001b[0m \u001b[39mfor\u001b[39;00m algorithm \u001b[39min\u001b[39;00m pipeline_step[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m--> 117\u001b[0m     current_model \u001b[39m=\u001b[39m algorithm\u001b[39m.\u001b[39;49mapply(current_model, current_graph, step_statistics)\n\u001b[0;32m    118\u001b[0m     current_graph \u001b[39m=\u001b[39m NNCFGraphFactory\u001b[39m.\u001b[39mcreate(current_model)\n\u001b[0;32m    119\u001b[0m current_model \u001b[39m=\u001b[39m pipeline_step[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mapply(current_model, current_graph, step_statistics)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\quantization\\algorithms\\min_max\\algorithm.py:717\u001b[0m, in \u001b[0;36mMinMaxQuantization.apply\u001b[1;34m(self, model, graph, statistic_points, dataset)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m transformation_layout\u001b[39m.\u001b[39mtransformations:\n\u001b[0;32m    716\u001b[0m     nncf_logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mThe model has no operations to apply quantization.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 717\u001b[0m quantized_model \u001b[39m=\u001b[39m model_transformer\u001b[39m.\u001b[39;49mtransform(transformation_layout)\n\u001b[0;32m    718\u001b[0m \u001b[39mreturn\u001b[39;00m quantized_model\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\model_transformer.py:67\u001b[0m, in \u001b[0;36mPTModelTransformer.transform\u001b[1;34m(self, transformation_layout)\u001b[0m\n\u001b[0;32m     64\u001b[0m         model \u001b[39m=\u001b[39m transformation_fn(model, transformations)\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m requires_graph_rebuild:\n\u001b[1;32m---> 67\u001b[0m     model\u001b[39m.\u001b[39;49mnncf\u001b[39m.\u001b[39;49mrebuild_graph()\n\u001b[0;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\nncf_network.py:501\u001b[0m, in \u001b[0;36mNNCFNetworkInterface.rebuild_graph\u001b[1;34m(self, *input_args)\u001b[0m\n\u001b[0;32m    498\u001b[0m builder \u001b[39m=\u001b[39m GraphBuilder(dummy_forward_fn)\n\u001b[0;32m    500\u001b[0m \u001b[39mwith\u001b[39;00m training_mode_switcher(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_ref, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compressed_graph \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39;49mbuild_graph(\n\u001b[0;32m    502\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model_ref, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compressed_context, input_infos\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_infos\n\u001b[0;32m    503\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\graph\\graph_builder.py:39\u001b[0m, in \u001b[0;36mGraphBuilder.build_graph\u001b[1;34m(self, model, context_to_use, as_eval, input_infos)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_graph\u001b[39m(\n\u001b[0;32m     32\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     33\u001b[0m     model: torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     input_infos: List[ModelInputInfo] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PTNNCFGraph:\n\u001b[0;32m     38\u001b[0m     tracer \u001b[39m=\u001b[39m GraphTracer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcustom_forward_fn)\n\u001b[1;32m---> 39\u001b[0m     dynamic_graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace_graph(model, context_to_use, as_eval)\n\u001b[0;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m GraphConverter\u001b[39m.\u001b[39mconvert(dynamic_graph, input_infos)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\dynamic_graph\\graph_tracer.py:114\u001b[0m, in \u001b[0;36mGraphTracer.trace_graph\u001b[1;34m(self, model, context_to_use, as_eval)\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcustom_forward_fn(model)\n\u001b[0;32m    113\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_forward_fn(model)\n\u001b[0;32m    115\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(sd)\n\u001b[0;32m    117\u001b[0m context_to_use\u001b[39m.\u001b[39mdisable_trace_dynamic_graph()\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\nncf_network.py:453\u001b[0m, in \u001b[0;36mNNCFNetworkInterface._get_dummy_forward_fn_for_graph_building.<locals>.wrapped_user_dummy_forward_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_user_dummy_forward_fn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    452\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_user_dummy_forward \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 453\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_user_dummy_forward_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    454\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_user_dummy_forward \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    455\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\quantize_model.py:73\u001b[0m, in \u001b[0;36mcreate_nncf_network.<locals>.create_dummy_forward_fn.<locals>.dummy_forward\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     70\u001b[0m     args \u001b[39m=\u001b[39m objwalk(args, is_tensor, send_to_device)\n\u001b[0;32m     71\u001b[0m     kwargs \u001b[39m=\u001b[39m objwalk(kwargs, is_tensor, send_to_device)\n\u001b[1;32m---> 73\u001b[0m args, kwargs \u001b[39m=\u001b[39m wrap_inputs(args, kwargs)\n\u001b[0;32m     74\u001b[0m retval \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m retval \u001b[39m=\u001b[39m replicate_same_tensors(retval)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\quantize_model.py:56\u001b[0m, in \u001b[0;36mcreate_nncf_network.<locals>.wrap_inputs\u001b[1;34m(args, kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_inputs\u001b[39m(args, kwargs):\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m wrap_nncf_model_inputs_with_objwalk(args, kwargs)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\dynamic_graph\\io_handling.py:46\u001b[0m, in \u001b[0;36mwrap_nncf_model_inputs_with_objwalk\u001b[1;34m(model_args, model_kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_nncf_model_inputs_with_objwalk\u001b[39m(model_args, model_kwargs):\n\u001b[1;32m---> 46\u001b[0m     model_args \u001b[39m=\u001b[39m objwalk(model_args, is_tensor, nncf_model_input)\n\u001b[0;32m     47\u001b[0m     model_kwargs \u001b[39m=\u001b[39m objwalk(model_kwargs, is_tensor, nncf_model_input)\n\u001b[0;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m model_args, model_kwargs\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\nested_objects_traversal.py:160\u001b[0m, in \u001b[0;36mobjwalk\u001b[1;34m(obj, unary_predicate, apply_fn, memo)\u001b[0m\n\u001b[0;32m    158\u001b[0m             objwalk(value, unary_predicate, apply_fn)\n\u001b[0;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices_to_apply_fn_to:\n\u001b[1;32m--> 160\u001b[0m     obj[idx] \u001b[39m=\u001b[39m apply_fn(obj[idx])\n\u001b[0;32m    161\u001b[0m \u001b[39mfor\u001b[39;00m idx, tpl_data \u001b[39min\u001b[39;00m indices_vs_named_tuple_data\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    162\u001b[0m     tpl, n_tpl_class, n_tpl_fields \u001b[39m=\u001b[39m tpl_data\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\dynamic_graph\\wrappers.py:95\u001b[0m, in \u001b[0;36mwrap_operator.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m     result \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mtensor_cache\n\u001b[0;32m     94\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     result \u001b[39m=\u001b[39m _execute_op(op_address, operator_info, operator, ctx, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     97\u001b[0m str_op_address \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(op_address)\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m str_op_address \u001b[39min\u001b[39;00m ctx\u001b[39m.\u001b[39mend_node_name_of_skipped_block:\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\dynamic_graph\\wrappers.py:192\u001b[0m, in \u001b[0;36m_execute_op\u001b[1;34m(op_address, operator_info, operator, ctx, *args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m         ctx\u001b[39m.\u001b[39mregister_node_call(node)\n\u001b[0;32m    191\u001b[0m result \u001b[39m=\u001b[39m trace_tensors(result, node, ctx)\n\u001b[1;32m--> 192\u001b[0m result \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39;49mexecute_post_hooks(op_address, result)\n\u001b[0;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\dynamic_graph\\context.py:291\u001b[0m, in \u001b[0;36mTracingContext.execute_post_hooks\u001b[1;34m(self, op_address, outputs)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m op_address \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_hooks:\n\u001b[0;32m    290\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_hooks[op_address]:\n\u001b[1;32m--> 291\u001b[0m         outputs \u001b[39m=\u001b[39m hook(outputs)\n\u001b[0;32m    292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threading\u001b[39m.\u001b[39mthread_local\u001b[39m.\u001b[39mnum_nested_hooks \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_operator \u001b[39m=\u001b[39m in_op\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\external_quantizer.py:40\u001b[0m, in \u001b[0;36mExternalQuantizerCallHook.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m replica \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompressed_context\u001b[39m.\u001b[39mbase_module_thread_local_replica\n\u001b[0;32m     39\u001b[0m storage \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(replica\u001b[39m.\u001b[39mnncf, EXTERNAL_QUANTIZERS_STORAGE_NAME)\n\u001b[1;32m---> 40\u001b[0m \u001b[39mreturn\u001b[39;00m storage[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquantizer_storage_key](\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\dynamic_graph\\wrappers.py:151\u001b[0m, in \u001b[0;36mwrap_module_call.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         ctx\u001b[39m.\u001b[39min_skipped_block \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     retval \u001b[39m=\u001b[39m module_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m ITERATION_MODULES\u001b[39m.\u001b[39mregistry_dict:\n\u001b[0;32m    154\u001b[0m     ctx\u001b[39m.\u001b[39mreset_operator_call_count_in_scope(ctx\u001b[39m.\u001b[39mscope)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\layers.py:391\u001b[0m, in \u001b[0;36mBaseQuantizer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[39m# The underlying operator (registered via register_operator) must be executed,\u001b[39;00m\n\u001b[0;32m    383\u001b[0m     \u001b[39m# otherwise the dynamic graph won't be traced as it was during regular inference.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[39m# While this does not impact the regular, non-RNN models, for which the graph\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[39m# this is important for LSTMs etc. where determining the \"first nodes in iteration\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[39m# scopes\" depends on whether the input tensors to an operation were traced or not.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantize(x, execute_traced_op_as_identity\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 391\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquantize(x, execute_traced_op_as_identity\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\layers.py:707\u001b[0m, in \u001b[0;36mSymmetricQuantizer.quantize\u001b[1;34m(self, x, execute_traced_op_as_identity)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquantize\u001b[39m(\u001b[39mself\u001b[39m, x, execute_traced_op_as_identity: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 707\u001b[0m     \u001b[39mreturn\u001b[39;00m symmetric_quantize(\n\u001b[0;32m    708\u001b[0m         x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlevels, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlevel_low, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlevel_high, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps, skip\u001b[39m=\u001b[39;49mexecute_traced_op_as_identity\n\u001b[0;32m    709\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\dynamic_graph\\wrappers.py:95\u001b[0m, in \u001b[0;36mwrap_operator.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m     result \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mtensor_cache\n\u001b[0;32m     94\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     result \u001b[39m=\u001b[39m _execute_op(op_address, operator_info, operator, ctx, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     97\u001b[0m str_op_address \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(op_address)\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m str_op_address \u001b[39min\u001b[39;00m ctx\u001b[39m.\u001b[39mend_node_name_of_skipped_block:\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\dynamic_graph\\wrappers.py:175\u001b[0m, in \u001b[0;36m_execute_op\u001b[1;34m(op_address, operator_info, operator, ctx, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(processed_input\u001b[39m.\u001b[39mop_args)\n\u001b[0;32m    174\u001b[0m kwargs \u001b[39m=\u001b[39m processed_input\u001b[39m.\u001b[39mop_kwargs\n\u001b[1;32m--> 175\u001b[0m result \u001b[39m=\u001b[39m operator(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    176\u001b[0m node \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtype\u001b[39m(\u001b[39mNotImplemented\u001b[39m)):\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\quantize_functions.py:199\u001b[0m, in \u001b[0;36msymmetric_quantize\u001b[1;34m(input_, levels, level_low, level_high, scale, eps, skip)\u001b[0m\n\u001b[0;32m    197\u001b[0m scale \u001b[39m=\u001b[39m scale\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39minput_\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    198\u001b[0m scale_safe \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(scale) \u001b[39m+\u001b[39m eps\n\u001b[1;32m--> 199\u001b[0m \u001b[39mreturn\u001b[39;00m QuantizeSymmetric\u001b[39m.\u001b[39;49mapply(input_, scale_safe, level_low, level_high, levels)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\torch\\autograd\\function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[0;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\quantize_functions.py:40\u001b[0m, in \u001b[0;36mQuantizeSymmetric.forward\u001b[1;34m(ctx, input_, scale, level_low, level_high, levels)\u001b[0m\n\u001b[0;32m     38\u001b[0m         input_low \u001b[39m=\u001b[39m input_low\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat16)\n\u001b[0;32m     39\u001b[0m         input_range \u001b[39m=\u001b[39m input_range\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat16)\n\u001b[1;32m---> 40\u001b[0m     output \u001b[39m=\u001b[39m QuantizedFunctionsCUDA\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mQuantize_forward\u001b[39;49m\u001b[39m\"\u001b[39;49m)(input_, input_low, input_range, levels)\n\u001b[0;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     output \u001b[39m=\u001b[39m QuantizedFunctionsCPU\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mQuantize_forward\u001b[39m\u001b[39m\"\u001b[39m)(input_, input_low, input_range, levels)\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\extensions\\__init__.py:102\u001b[0m, in \u001b[0;36mExtensionNamespace.get\u001b[1;34m(self, fn_name)\u001b[0m\n\u001b[0;32m    100\u001b[0m     pool \u001b[39m=\u001b[39m ThreadPool(processes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    101\u001b[0m     async_result \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39mapply_async(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader\u001b[39m.\u001b[39mload)\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loaded_namespace \u001b[39m=\u001b[39m async_result\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    103\u001b[0m \u001b[39mexcept\u001b[39;00m MPTimeoutError \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m    104\u001b[0m     msg \u001b[39m=\u001b[39m textwrap\u001b[39m.\u001b[39mdedent(\n\u001b[0;32m    105\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39m        The extension load function failed to execute within \u001b[39m\u001b[39m{\u001b[39;00mtimeout\u001b[39m}\u001b[39;00m\u001b[39m seconds.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     )\n",
            "File \u001b[1;32mC:\\Python311\\Lib\\multiprocessing\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
            "File \u001b[1;32mC:\\Python311\\Lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    123\u001b[0m job, i, func, args, kwds \u001b[39m=\u001b[39m task\n\u001b[0;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m wrap_exception \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _helper_reraises_exception:\n",
            "File \u001b[1;32mc:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\extensions.py:97\u001b[0m, in \u001b[0;36mQuantizedFunctionsCUDALoader.load\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mexcept\u001b[39;00m (subprocess\u001b[39m.\u001b[39mCalledProcessError, \u001b[39mOSError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     96\u001b[0m     \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available()\n\u001b[1;32m---> 97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     98\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCUDA is available for PyTorch, but NNCF could not compile \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGPU quantization extensions. Make sure that you have installed CUDA development \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtools (see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mguidance) and that \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnvcc\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is available on your system\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms PATH variable.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA is available for PyTorch, but NNCF could not compile GPU quantization extensions. Make sure that you have installed CUDA development tools (see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html for guidance) and that 'nvcc' is available on your system's PATH variable.\n"
          ]
        }
      ],
      "source": [
        "def transform_fn(data_item):\n",
        "    images, _ = data_item\n",
        "    return images\n",
        "\n",
        "calibration_dataset = nncf.Dataset(train_loader, transform_fn)\n",
        "quantized_model = nncf.quantize(model, calibration_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-khBK9_BfQ1"
      },
      "source": [
        "Finally, we will convert models to OpenVINO Intermediate Representation (IR) format.\n",
        "\n",
        "OpenVINO IR is the proprietary model format of OpenVINO. It is produced after converting a model with model conversion API. It translates the frequently used deep learning operations to their respective similar representation in OpenVINO and tunes them with the associated weights and biases from the trained model. The resulting IR contains two files:\n",
        "- `xml` - Describes the model topology.\n",
        "- `bin` - Contains the weights and binary data.\n",
        "\n",
        "To do that, we'll need `dummy_input` filled with random values and of size:\n",
        "\n",
        "`[batch_size, channel_number, image_shape[0], image_shape[1]]`\n",
        "\n",
        "Create `MNIST_fp32_ir` model with `ov.convert_model` that takes three parameters: the model, the dummy input and input size. Use `CNN_MNIST` model.\n",
        "\n",
        "Then, create `MNIST_int8_ir` model in the same way using `quantized_model`.\n",
        "\n",
        "Save both models to files (named `MNIST_fp32_ir.xml` and `MNIST_int8_ir.xml` respectively). Use `ov.save_model()` function.\n",
        "\n",
        "Finally - compile both models with `core.compile_model` function and use  `validate` function to calculate both models' accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CcYxirUfBV1",
        "outputId": "002b0d64-0e53-4b52-8cc1-5ae951836d61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "core = ov.Core()\n",
        "devices = core.available_devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'CPU'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "devices[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2BkQcrOUcn8R",
        "outputId": "ff967f1d-f1f3-42b1-eeea-661aad2b53ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\layers.py:336: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  return self._level_low.item()\n",
            "c:\\Informatique\\EmbeddedAI\\env\\Lib\\site-packages\\nncf\\torch\\quantization\\layers.py:344: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  return self._level_high.item()\n",
            "313it [00:01, 169.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FP 32 model acc=0.6207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "313it [00:01, 219.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INT 8 model acc=0.6174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import openvino as ov\n",
        "from openvino import CompiledModel\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Union\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "\n",
        "core = ov.Core()\n",
        "dummy_input = torch.randn(1, 1, 28, 28)\n",
        "\n",
        "MNIST_fp32_ir = ov.convert_model(model, example_input=dummy_input, input=[-1, 1, 28, 28])\n",
        "\n",
        "MNIST_int8_ir = ov.convert_model(quantized_model, example_input=dummy_input, input=[-1, 1, 28, 28])\n",
        "\n",
        "ov.save_model(MNIST_fp32_ir, \"mnist_fp32.xml\")  # Fill the filename\n",
        "ov.save_model(MNIST_int8_ir, \"mnist_int8.xml\")  # Fill the filename\n",
        "\n",
        "fp32_compiled_model = core.compile_model(MNIST_fp32_ir, \"CPU\")\n",
        "int8_compiled_model = core.compile_model(MNIST_int8_ir, \"CPU\")\n",
        "\n",
        "def validate(val_loader: torch.utils.data.DataLoader, model: Union[torch.nn.Module, CompiledModel], metric: BaseMetric):\n",
        "    # Switch to evaluate mode.\n",
        "    if not isinstance(model, CompiledModel):\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "\n",
        "    total_accuracy = 0\n",
        "    samples_num = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in tqdm.tqdm(enumerate(val_loader)):\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # Ensure the model is on the correct device\n",
        "            if not isinstance(model, CompiledModel):\n",
        "                model.to(device)\n",
        "\n",
        "            # Compute the output.\n",
        "            if isinstance(model, CompiledModel):\n",
        "                output_layer = model.output(0)\n",
        "                output = model(images)[output_layer]\n",
        "                output= torch.from_numpy(output)\n",
        "            else:\n",
        "                output = model(images)\n",
        "\n",
        "            # Measure accuracy and record loss.\n",
        "            accuracy = metric(output, target)\n",
        "            total_accuracy += accuracy.item() * target.shape[0]\n",
        "            samples_num += target.shape[0]\n",
        "\n",
        "    return total_accuracy / samples_num\n",
        "\n",
        "\n",
        "# Assuming you have a DataLoader for validation named 'val_loader'\n",
        "acc1 = validate(val_loader=test_loader, model=fp32_compiled_model, metric=AccuracyMetric())\n",
        "print(f'FP 32 model acc={acc1:.4f}')\n",
        "\n",
        "acc2 = validate(val_loader=test_loader, model=int8_compiled_model, metric=AccuracyMetric())\n",
        "print(f'INT 8 model acc={acc2:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpb_tq9yEKBg"
      },
      "source": [
        "Is INT8 model accuracy similar to FP32 model accuracy? We should hope so!\n",
        "\n",
        "But let's verify what we have saved in terms of memory resources and network throughput!\n",
        "\n",
        "First, check the size of OpenVINO IR binary files. You saved both of them on your drive. Is the INT8 model smaller?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have 244ko vs 120ko"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vp6wdbtFD6C"
      },
      "source": [
        "Then, use the following code to benchmark both models. Is INT8 model faster?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "313it [00:03, 94.01it/s] \n",
        "FP 32 model acc=0.6372\n",
        "313it [00:02, 146.47it/s]\n",
        "INT 8 model acc=0.6374\n",
        "\n",
        "INT 8 is faster !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPe2aECWFmqF"
      },
      "source": [
        "Note that we used very small network and we deal with very simple task. For bigger models and more complicated networks the perfomance and size differences can be even more significant!\n",
        "\n",
        "**EXTENSION EXERCISE**\n",
        "\n",
        "Read about `Quantizing with Accuracy Control` and try to use it for some pretrained network. Use `nncf.quantize_with_accuracy_control`. You can find pretrained networks with `torchvision.models`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpPQPePcGoDL"
      },
      "source": [
        "## Quantization-aware Training (QAT)\n",
        "\n",
        "Training-time model compression improves model performance by applying optimizations (such as quantization) during the training. The training process minimizes the loss associated with the lower-precision optimizations, so it is able to maintain the model’s accuracy while reducing its latency and memory footprint. Generally, training-time model optimization results in better model performance and accuracy than post-training optimization, but it can require more effort to set up.\n",
        "\n",
        "Quantization-aware Training is a popular method that allows quantizing a model and applying fine-tuning to restore accuracy degradation caused by quantization. In fact, this is the most accurate quantization method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ns4TqBHTM_"
      },
      "source": [
        "For this part, let's use a bit harder Dataset. For MNIST, PTQ method was enough, right?\n",
        "\n",
        "Train your CNN model on CIFAR10 dataset for 10-20 epochs (google it!). Use the same training loops, metrics, optimizers and loss function.\n",
        "\n",
        "Name the final trained model `CNN_CIFAR`, convert it to OpenVINO IR and save to an xml file.\n",
        "\n",
        "We start our QAT process with creating compressed models. Just use the following code (fill in the gaps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xdNDbJp09l3Q"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'training' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Informatique\\EmbeddedAI\\Embedded\\lab3\\Lab3.ipynb Cell 26\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_loader \u001b[39m=\u001b[39m \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_loader \u001b[39m=\u001b[39m \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m CNN_CIFAR, history \u001b[39m=\u001b[39m training( \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# SAVE floating point model converted to OpenVINO IR\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Informatique/EmbeddedAI/Embedded/lab3/Lab3.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m) \u001b[39m# Create dummy_input\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'training' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import openvino as ov\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Replace this with your actual model\n",
        "CNN_CIFAR = model\n",
        "\n",
        "# SAVE floating point model converted to OpenVINO IR\n",
        "dummy_input = torch.randn(1, 3, 32, 32)  # Replace with your actual input shape\n",
        "CIFAR_fp32_ir = ov.convert_model(CNN_CIFAR, dummy_input)\n",
        "ov.save_model(CIFAR_fp32_ir, 'path/to/your/fp32_model.xml')  # Replace with your desired file path\n",
        "\n",
        "# Compress model\n",
        "nncf_config_dict = {\n",
        "    \"input_info\": {\"sample_size\": [1, 3, 32, 32]},  # Adjust with actual input size\n",
        "    \"compression\": {\n",
        "        \"algorithm\": \"quantization\",\n",
        "    },\n",
        "}\n",
        "nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n",
        "nncf_config = register_default_init_args(nncf_config, train_loader)\n",
        "compression_ctrl, CNN_CIFAR_int8 = create_compressed_model(CNN_CIFAR, nncf_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wetn5RnaZzGx"
      },
      "source": [
        "We have our CIFAR CNN model ready to QAT. So... Just train it!\n",
        "\n",
        "Use your `training` function to train `CNN_CIFAR_int8` model for one more epoch!\n",
        "\n",
        "Thanks to OpenVINO API, after creating compressed model all we need to do is to continue training on INT8 model :) We call this process fine-tuning. It is applied to futher improve quantized model accuracy! Normally, several epochs of tuning are required with a small learning rate, the same that is usually used at the end of the training of the original model. No other changes in the training pipeline are required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj2Va0bwaW0i"
      },
      "outputs": [],
      "source": [
        "CNN_CIFAR_int8_finetuned, history = training( ... ) # just one epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKp3LbjUbBg-"
      },
      "source": [
        "Convert fine-tuned model to OpenVINO IR, save it to xml and verify both `CIFAR_fp32_ir` and `CIFAR_int8_ir` sizes.\n",
        "\n",
        "Is the INT8 network smaller?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MigLfK-pLVqA"
      },
      "outputs": [],
      "source": [
        "core = ov.Core()\n",
        "devices = core.available_devices\n",
        "dummy_input = ...\n",
        "CIFAR_int8_ir = ov.convert_model( ... )\n",
        "ov.save_model( ... )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS9CujUULnz0"
      },
      "outputs": [],
      "source": [
        "!ls -lh ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbvVrwcCbZH_"
      },
      "source": [
        "Finally - compile models, validate and benchmark them.\n",
        "\n",
        "Did accuracy decreased?\n",
        "Is INT8 model faster?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1gbA8jbLidw"
      },
      "outputs": [],
      "source": [
        "fp32_cifar_compiled_model = core.compile_model( ... )\n",
        "int8_cifar_compiled_model = core.compile_model( ... )\n",
        "acc1 = validate( ... )\n",
        "print( ... )\n",
        "acc2 = validate( ... )\n",
        "print( ... )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pmPBzNrbm7r"
      },
      "outputs": [],
      "source": [
        "def parse_benchmark_output(benchmark_output: str):\n",
        "    \"\"\"Prints the output from benchmark_app in human-readable format\"\"\"\n",
        "    parsed_output = [line for line in benchmark_output if 'FPS' in line]\n",
        "    print(*parsed_output, sep='\\n')\n",
        "\n",
        "\n",
        "print('Benchmark FP32 model on CPU')\n",
        "benchmark_output = ! benchmark_app -m CIFAR_fp32_ir.xml -d CPU -api async -t 15 -shape \"[1, 3, 32, 32]\"\n",
        "parse_benchmark_output(benchmark_output)\n",
        "\n",
        "print('Benchmark INT8 model on CPU')\n",
        "benchmark_output = ! benchmark_app -m CIFAR_int8_ir.xml -d CPU -api async -t 15 -shape \"[1, 3, 32, 32]\"\n",
        "parse_benchmark_output(benchmark_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wku_Ng6WcDdI"
      },
      "source": [
        "**EXTENSION EXERCISE**\n",
        "\n",
        "Compare PTQ and QAT. Create CNN model and:\n",
        "- train it for 20 epochs and save as `CNN_long.pth`\n",
        "- train it for 15 epochs and save as `CNN_short.pth`\n",
        "\n",
        "Then, apply PTQ on `CNN_long.pth` model and QAT (for 5 epochs) on `CNN_short.pth`. Compare the resulting models (in terms of accuracy, size and FPS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY_MYT8GFR-u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
